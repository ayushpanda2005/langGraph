{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joke Generator + Time Travel Explainer\n",
    "\n",
    "This notebook shows a simple **joke generator** (topic → write joke) and then focuses on **LangGraph time travel**: how to go back to a previous checkpoint *after* the joke is generated, inspect or change state, and resume to get alternative outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from typing import TypedDict, NotRequired\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. State and nodes\n",
    "\n",
    "Simple state: `topic` (from LLM) and `joke` (written from that topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeState(TypedDict):\n",
    "    topic: NotRequired[str]\n",
    "    joke: NotRequired[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic(state: JokeState):\n",
    "    \"\"\"LLM picks a funny topic for the joke.\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You suggest short, funny topics for jokes. Reply with only the topic, no explanation.\"),\n",
    "        HumanMessage(content=\"Give me one funny topic for a joke.\")\n",
    "    ]\n",
    "    topic = llm.invoke(messages).content\n",
    "    return {\"topic\": topic}\n",
    "\n",
    "def write_joke(state: JokeState):\n",
    "    \"\"\"LLM writes a short joke based on the topic.\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You write short, clean jokes. No setup-punchline clichés unless they're good.\"),\n",
    "        HumanMessage(content=f\"Write a short joke about: {state['topic']}\")\n",
    "    ]\n",
    "    joke = llm.invoke(messages).content\n",
    "    return {\"joke\": joke}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(JokeState)\n",
    "workflow.add_node(\"generate_topic\", generate_topic)\n",
    "workflow.add_node(\"write_joke\", write_joke)\n",
    "workflow.add_edge(START, \"generate_topic\")\n",
    "workflow.add_edge(\"generate_topic\", \"write_joke\")\n",
    "workflow.add_edge(\"write_joke\", END)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=checkpointer)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the graph (joke generation)\n",
    "\n",
    "We **must** pass a `thread_id` in config so the checkpointer can save every step. That’s what makes time travel possible later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"joke-demo-1\"}}\n",
    "final_state = graph.invoke({}, config)\n",
    "\n",
    "print(\"Topic:\", final_state[\"topic\"])\n",
    "print()\n",
    "print(\"Joke:\", final_state[\"joke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Time travel (focus: *after* joke generation)\n",
    "\n",
    "**What is time travel here?**  \n",
    "LangGraph saves a **checkpoint** at every super-step. Each checkpoint is a snapshot of the graph state and \"what runs next.\" **Time travel** means:\n",
    "\n",
    "1. **Inspect the past** – List checkpoints for this run and see state at any step.\n",
    "2. **Resume from a checkpoint** – Run again from that point (same or updated state), creating a **new fork** in history.\n",
    "\n",
    "So *after* the joke is generated we can:\n",
    "- Look at the **history** of checkpoints.\n",
    "- Pick a checkpoint (e.g. *after* `generate_topic` but *before* `write_joke`).\n",
    "- Optionally **update state** at that checkpoint (e.g. change `topic`).\n",
    "- **Resume** from that checkpoint → only steps *after* it run again (e.g. `write_joke` runs with the new topic).\n",
    "\n",
    "That’s the part we demo below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 List checkpoints (state history)\n",
    "\n",
    "`get_state_history(config)` returns all checkpoints for this `thread_id`, **newest first**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(graph.get_state_history(config))\n",
    "print(f\"Total checkpoints: {len(history)}\")\n",
    "for i, snap in enumerate(history):\n",
    "    next_nodes = snap.next\n",
    "    cid = snap.config[\"configurable\"].get(\"checkpoint_id\", \"N/A\")[:8]\n",
    "    print(f\"  {i}: next={next_nodes}  checkpoint_id=...{cid}\")\n",
    "    if snap.values:\n",
    "        keys = list(snap.values.keys())\n",
    "        print(f\"      state keys: {keys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pick a checkpoint: *after* topic, *before* joke\n",
    "\n",
    "We want the snapshot where `generate_topic` has run (we have `topic`) but `write_joke` hasn’t. In the list above, that’s the one whose `next` is `('write_joke',)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = None\n",
    "for snap in history:\n",
    "    if snap.next == (\"write_joke\",):\n",
    "        selected = snap\n",
    "        break\n",
    "\n",
    "if selected is None:\n",
    "    selected = history[1]  # fallback: second (older) checkpoint\n",
    "\n",
    "print(\"Selected checkpoint: next =\", selected.next)\n",
    "print(\"State at this point:\", selected.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Update state and resume (fork)\n",
    "\n",
    "We **change** the topic at this checkpoint with `update_state`. That creates a **new** checkpoint (new `checkpoint_id`). Then we **resume** with `invoke(None, new_config)` so the graph runs *from that point onward* — i.e. only `write_joke` runs again, with the new topic. Result: a **different joke** from the same \"time\" in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topic = \"programmers and coffee\"\n",
    "new_config = graph.update_state(selected.config, values={\"topic\": new_topic})\n",
    "\n",
    "forked_state = graph.invoke(None, new_config)\n",
    "\n",
    "print(\"Forked run (after time travel + topic change):\")\n",
    "print(\"Topic:\", forked_state[\"topic\"])\n",
    "print()\n",
    "print(\"Joke:\", forked_state[\"joke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Resume without changing state (replay)\n",
    "\n",
    "You can also **replay** from a checkpoint without updating state: pass a config that includes that checkpoint’s `checkpoint_id` and call `invoke(None, config)`. The graph will re-execute from that point (e.g. `write_joke` runs again with the *same* topic — may give a different joke due to LLM non-determinism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "        \"checkpoint_id\": selected.config[\"configurable\"][\"checkpoint_id\"]\n",
    "    }\n",
    "}\n",
    "replayed_state = graph.invoke(None, replay_config)\n",
    "\n",
    "print(\"Replayed run (same topic, from checkpoint):\")\n",
    "print(\"Topic:\", replayed_state[\"topic\"])\n",
    "print()\n",
    "print(\"Joke:\", replayed_state[\"joke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Step | What we did |\n",
    "|------|-------------|\n",
    "| 1 | Ran the joke graph with a `thread_id` so every step is checkpointed. |\n",
    "| 2 | Used **`get_state_history(config)`** to list all checkpoints (newest first). |\n",
    "| 3 | Chose a checkpoint **after** `generate_topic` and **before** `write_joke`. |\n",
    "| 4 | **Time travel + fork:** `update_state(selected.config, values={...})` then `invoke(None, new_config)` to get a new joke with a different topic. |\n",
    "| 5 | **Replay:** `invoke(None, config_with_checkpoint_id)` to re-run from that point without changing state. |\n",
    "\n",
    "So **time travel** here = go back to a saved checkpoint (after joke generation has already run once), optionally edit state, and resume to create an alternative branch. All of this is built on **persistence** (checkpointer + `thread_id`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
